{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping for Images\n",
    "This script will be used to scrape images for the following types of stemware:\n",
    "- champagne flutes\n",
    "- martini glass (formally cocktail glass)\n",
    "- rummer glass\n",
    "- snifters\n",
    "- red wine glasses\n",
    "- white wine glasses\n",
    "- sherry glasses (schooners)\n",
    "\n",
    "The images will then be passed through a Mechanical Turk-type process (Manual Turk) for label validation. Label validation is important for this exercise as stemware has a wide variety to them. It'll be important to keep our recognition engine to a standard stemware collection and leave the other items to their own devices.\n",
    "\n",
    "Having a large sample size is also important. I believe a sample size of more than 1,000 images per category will lead to high accuracy, but the goal for now is to start at 200 images per category and use image adjustment techniques (see Jupyter Notebook: 'Image Adjustment').\n",
    "\n",
    "Chalice/Goblets have been removed due to their ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bing Scraping\n",
    "The unfortunate result of this query is a small set of 35 images. Not enough for our needs, but all data points are useful.\n",
    "\n",
    "Resources: https://gist.github.com/stephenhouser/c5e2b921c3770ed47eb3b75efbc94799 ; https://stackoverflow.com/questions/18497840/beautifulsoup-how-to-open-images-and-download-them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import json\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bing_soup(searchterm):\n",
    "    #uses the search term as the url search\n",
    "    term = searchterm\n",
    "    term= term.split()\n",
    "    term='+'.join(term)\n",
    "    url=\"http://www.bing.com/images/search?q=\" + str(term) + \"&FORM=HDRSC2\"\n",
    "    #creates a folder for the search term\n",
    "    DIR=\"C:\\\\Users\\\\chris\\\\Documents\\\\CUNY\\\\DATA698 - Final Masters Thesis\\\\Images\\\\\" + term\n",
    "    if not os.path.exists(DIR):\n",
    "        os.mkdir(DIR)\n",
    "    #header will imitate a user\n",
    "    header={'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36\"}\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(urllib.request.Request(url, headers=header)), 'html.parser')\n",
    "    #will be used to save image details\n",
    "    ActualImages=[]\n",
    "    \n",
    "    for a in soup.find_all(\"a\", {\"class\":\"iusc\"}):\n",
    "        #m contains murl which contains image url and name\n",
    "        m =json.loads(a[\"m\"])\n",
    "        murl = m[\"murl\"]\n",
    "        image_name = urllib.parse.urlsplit(murl).path.split(\"/\")[-1]\n",
    "        #mad contains turl which contains original image url\n",
    "        mad=json.loads(a[\"mad\"])\n",
    "        turl = mad[\"turl\"]\n",
    "        ActualImages.append((image_name, turl, murl))\n",
    "    print(\"there are total\" , len(ActualImages), \"images\")\n",
    "    \n",
    "    for i, (image_name, turl, murl) in enumerate(ActualImages):\n",
    "        try:\n",
    "            raw_img = urllib.request.urlopen(turl).read()\n",
    "            f = open(os.path.join(DIR, image_name), 'wb')\n",
    "            f.write(raw_img)\n",
    "            f.close()\n",
    "        except Exception as e:\n",
    "            print(\"could not load : \" + image_name)\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are total 35 images\n",
      "there are total 35 images\n",
      "there are total 35 images\n",
      "there are total 35 images\n",
      "there are total 35 images\n",
      "there are total 35 images\n",
      "there are total 35 images\n"
     ]
    }
   ],
   "source": [
    "#added sleep to give a time delay of 10 seconds before query\n",
    "get_bing_soup('champagne flute')\n",
    "sleep(10)\n",
    "get_bing_soup('martini glass')\n",
    "sleep(10)\n",
    "get_bing_soup('rummer glass')\n",
    "sleep(10)\n",
    "get_bing_soup('snifters')\n",
    "sleep(10)\n",
    "get_bing_soup('red wine glass')\n",
    "sleep(10)\n",
    "get_bing_soup('white wine glass')\n",
    "sleep(10)\n",
    "get_bing_soup('sherry glass')\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Images Search\n",
    "https://stackoverflow.com/questions/20716842/python-download-images-from-google-image-search\n",
    "https://github.com/CumminUp07/imengine/blob/master/get_google_images.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request, urllib.error, urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_google_soup(searchterm):\n",
    "    #uses the search term as the url search\n",
    "    term = searchterm\n",
    "    term= term.split()\n",
    "    term='+'.join(term)\n",
    "    url=\"https://www.google.com/search?q=\" + str(term) + \"&source=lnms&tbm=isch\"\n",
    "    #creates a folder for the search term\n",
    "    DIR=\"C:\\\\Users\\\\chris\\\\Documents\\\\CUNY\\\\DATA698 - Final Masters Thesis\\\\Images\\\\\" + term\n",
    "    if not os.path.exists(DIR):\n",
    "        os.mkdir(DIR)\n",
    "    #header will imitate a user\n",
    "    header={'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36\"}\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(urllib.request.Request(url, headers=header)), 'html.parser')\n",
    "    ActualImages=[]\n",
    "    \n",
    "    #pulls image name, link, and image type\n",
    "    for a in soup.find_all(\"div\",{\"class\":\"rg_meta\"}):\n",
    "        link, Type =json.loads(a.text)[\"ou\"], json.loads(a.text)[\"ity\"]\n",
    "        #added google to identify source\n",
    "        image_name = urllib.parse.urlsplit(link).path.split(\"/\")[-1] + '_google'\n",
    "        ActualImages.append((image_name, link, Type))\n",
    "    print(\"there are total\" , len(ActualImages), \"images\")\n",
    "    \n",
    "    #defaults image type\n",
    "    image_type=\"ActiOn\"\n",
    "    \n",
    "    #saves images using jpg or the default image type if not jpg\n",
    "    try:\n",
    "        for i, (image_name, img, Type) in enumerate(ActualImages):\n",
    "            raw_img = urllib.request.urlopen(urllib.request.Request(img, headers=header)).read()\n",
    "            cntr = len([i for i in os.listdir(DIR) if image_type in i]) + 1\n",
    "            if len(Type)==0:\n",
    "                f=open(os.path.join(DIR, image_name + \".jpg\"), 'wb')\n",
    "            else:\n",
    "                f=open(os.path.join(DIR, image_name + \".\" + Type), 'wb')\n",
    "\n",
    "            f.write(raw_img)\n",
    "            f.close()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"could not load : \" + img)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are total 100 images\n",
      "there are total 100 images\n",
      "there are total 100 images\n",
      "there are total 100 images\n",
      "could not load : https://images.prod.meredith.com/product/9d547fb10349ba6c2f7dbf68cde9498a/1504157618842/l/orrefors-prestige-set-of-4-cognac-snifters\n",
      "HTTP Error 403: Forbidden\n",
      "there are total 100 images\n",
      "there are total 100 images\n",
      "there are total 100 images\n",
      "could not load : https://images.prod.meredith.com/product/061163df1312777a58ba58b729732efd/1507781871925/l/baccarat-perfection-sherry-glass-plain\n",
      "HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "#added sleep to give a time delay of 10 seconds before query\n",
    "get_google_soup('champagne flute')\n",
    "sleep(10)\n",
    "get_google_soup('martini glass')\n",
    "sleep(10)\n",
    "get_google_soup('rummer glass')\n",
    "sleep(10)\n",
    "get_google_soup('snifters')\n",
    "sleep(10)\n",
    "get_google_soup('red wine glass')\n",
    "sleep(10)\n",
    "get_google_soup('white wine glass')\n",
    "sleep(10)\n",
    "get_google_soup('sherry glass')\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo Image Search\n",
    "\n",
    "NEED URL TO SEARCH APPROPRIATELY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://images.search.yahoo.com/search/images;_ylt=AwrWnfY47OdaaHEAV3iLuLkF;_ylc=X1MDOTYwNTc0ODMEX3IDMgRiY2sDOHN2OTNndGQ2cWg2ciUyNmIlM0QzJTI2cyUzRDVnBGZyAwRncHJpZANWQjFVYXJ0dFJHeTVVeERkemJXX0dBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE1BHF1ZXJ5A2NoYW1wYWduZSBmbHV0ZQR0X3N0bXADMTUyNTE0ODc0NwR2dGVzdGlkA251bGw-?gprid=VB1UarttRGy5UxDdzbW_GA&pvid=a4Ft6jEwLjKOfSOHWm1E2wLANzMuNwAAAABcL.ky&fr2=sb-top-images.search.yahoo.com&p=champagne+flute&ei=UTF-8&iscqry=&fr=sfp#id=0&iurl=http%3A%2F%2Fwww.urbanbar.com%2Fwp-content%2Fuploads%2F2016%2F11%2FUB3246-2.png&action=close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_yahoo_soup(searchterm):\n",
    "    #uses the search term as the url search\n",
    "    term = searchterm\n",
    "    term= term.split()\n",
    "    term='+'.join(term)\n",
    "    url=\"https://www.google.com/search?q=\" + str(term) + \"&source=lnms&tbm=isch\"\n",
    "    #creates a folder for the search term\n",
    "    DIR=\"C:\\\\Users\\\\chris\\\\Documents\\\\CUNY\\\\DATA698 - Final Masters Thesis\\\\Images\\\\\" + term\n",
    "    if not os.path.exists(DIR):\n",
    "        os.mkdir(DIR)\n",
    "    #header will imitate a user\n",
    "    header={'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36\"}\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(urllib.request.Request(url, headers=header)), 'html.parser')\n",
    "    ActualImages=[]\n",
    "    \n",
    "    #pulls image name, link, and image type\n",
    "    for a in soup.find_all(\"div\",{\"class\":\"rg_meta\"}):\n",
    "        link, Type =json.loads(a.text)[\"ou\"], json.loads(a.text)[\"ity\"]\n",
    "        image_name = urllib.parse.urlsplit(link).path.split(\"/\")[-1] + ''\n",
    "        ActualImages.append((image_name, link, Type))\n",
    "    print(\"there are total\" , len(ActualImages), \"images\")\n",
    "    \n",
    "    #defaults image type\n",
    "    image_type=\"ActiOn\"\n",
    "    \n",
    "    #saves images using jpg or the default image type if not jpg\n",
    "    try:\n",
    "        for i, (image_name, img, Type) in enumerate(ActualImages):\n",
    "            raw_img = urllib.request.urlopen(urllib.request.Request(img, headers=header)).read()\n",
    "            cntr = len([i for i in os.listdir(DIR) if image_type in i]) + 1\n",
    "            if len(Type)==0:\n",
    "                f=open(os.path.join(DIR, image_name + \".jpg\"), 'wb')\n",
    "            else:\n",
    "                f=open(os.path.join(DIR, image_name + \".\" + Type), 'wb')\n",
    "\n",
    "            f.write(raw_img)\n",
    "            f.close()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"could not load : \" + img)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-7ab9f6320ee2>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-7ab9f6320ee2>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    https://images.search.yahoo.com/search/images;_ylt=AwrWnfY47OdaaHEAV3iLuLkF;_ylc=X1MDOTYwNTc0ODMEX3IDMgRiY2sDOHN2OTNndGQ2cWg2ciUyNmIlM0QzJTI2cyUzRDVnBGZyAwRncHJpZANWQjFVYXJ0dFJHeTVVeERkemJXX0dBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE1BHF1ZXJ5A2NoYW1wYWduZSBmbHV0ZQR0X3N0bXADMTUyNTE0ODc0NwR2dGVzdGlkA251bGw-?gprid=VB1UarttRGy5UxDdzbW_GA&pvid=a4Ft6jEwLjKOfSOHWm1E2wLANzMuNwAAAABcL.ky&fr2=sb-top-images.search.yahoo.com&p=champagne+flute&ei=UTF-8&iscqry=&fr=sfp#id=0&iurl=http%3A%2F%2Fwww.urbanbar.com%2Fwp-content%2Fuploads%2F2016%2F11%2FUB3246-2.png&action=close\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "term = 'dogs'\n",
    "term= term.split()\n",
    "term='+'.join(term)\n",
    "url=\"https://images.search.yahoo.com&p=\" + str(term) + \"&ei=UTF-8&iscqry=&fr=sfp\"\n",
    "#creates a folder for the search term\n",
    "DIR=\"C:\\\\Users\\\\chris\\\\Documents\\\\CUNY\\\\DATA698 - Final Masters Thesis\\\\Images\\\\\" + term\n",
    "header={'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36\"}\n",
    "soup = BeautifulSoup(urllib.request.urlopen(urllib.request.Request(url, headers=header)), 'html.parser')\n",
    "ActualImages=[]\n",
    "https://images.search.yahoo.com/search/images;_ylt=AwrWnfY47OdaaHEAV3iLuLkF;_ylc=X1MDOTYwNTc0ODMEX3IDMgRiY2sDOHN2OTNndGQ2cWg2ciUyNmIlM0QzJTI2cyUzRDVnBGZyAwRncHJpZANWQjFVYXJ0dFJHeTVVeERkemJXX0dBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzE1BHF1ZXJ5A2NoYW1wYWduZSBmbHV0ZQR0X3N0bXADMTUyNTE0ODc0NwR2dGVzdGlkA251bGw-?gprid=VB1UarttRGy5UxDdzbW_GA&pvid=a4Ft6jEwLjKOfSOHWm1E2wLANzMuNwAAAABcL.ky&fr2=sb-top-images.search.yahoo.com&p=champagne+flute&ei=UTF-8&iscqry=&fr=sfp#id=0&iurl=http%3A%2F%2Fwww.urbanbar.com%2Fwp-content%2Fuploads%2F2016%2F11%2FUB3246-2.png&action=close\n",
    "https://images.search.yahoo.com/search/images;_ylt=AwrUi5xn7uda4kIAxASLuLkF;_ylc=X1MDOTYwNTc0ODMEX3IDMgRiY2sDOHN2OTNndGQ2cWg2ciUyNmIlM0QzJTI2cyUzRDVnBGZyAwRncHJpZANQTUJmNnU0VFN1ZUxadnN0ZkFvUDVBBG10ZXN0aWQDbnVsbARuX3N1Z2cDMTAEb3JpZ2luA2ltYWdlcy5zZWFyY2gueWFob28uY29tBHBvcwMwBHBxc3RyAwRwcXN0cmwDBHFzdHJsAzMEcXVlcnkDZG9nBHRfc3RtcAMxNTI1MzIxNTY2BHZ0ZXN0aWQDbnVsbA--?gprid=PMBf6u4TSueLZvstfAoP5A&pvid=uUi.xzEwLjKOfSOHWm1E2wAeNzMuNwAAAAB9hL0i&fr2=sb-top-images.search.yahoo.com&p=dog&ei=UTF-8&iscqry=&fr=sfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yandex\n",
    "Yandex is a Russian company that owns Yandex Search generating over half of search traffic in Russia. As such, it is a valuable resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error, urllib.parse\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_yandex_soup(searchterm):\n",
    "    #uses the search term as the url search\n",
    "    term = searchterm\n",
    "    term= term.split()\n",
    "    term='+'.join(term)\n",
    "    url=\"https://yandex.com/images/search?text=\" + str(term)\n",
    "    #creates a folder for the search term\n",
    "    DIR=\"C:\\\\Users\\\\chris\\\\Documents\\\\CUNY\\\\DATA698 - Final Masters Thesis\\\\Images\\\\\" + term\n",
    "    if not os.path.exists(DIR):\n",
    "        os.mkdir(DIR)\n",
    "    #header will imitate a user\n",
    "    header={'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36\"}\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(urllib.request.Request(url, headers=header)), 'html.parser')\n",
    "    ActualImages=[]\n",
    "\n",
    "    #pulls image name, link, and image type\n",
    "    for a in soup.find_all(\"a\",{\"class\":\"serp-item__link\"}):\n",
    "        link = a.get('href')\n",
    "        link = re.search('(?<=img_url\\=).*', link).group()\n",
    "        link = unquote(link)\n",
    "        link = re.sub('(\\&pos).*', '', link)\n",
    "        link = re.sub('\\\\', '', link)\n",
    "        name = a.find('img')\n",
    "        #added yandex to name in case of duplicates\n",
    "        name = name['alt'] + 'yandex'\n",
    "        ActualImages.append((link, name))\n",
    "    print(\"there are total\" , len(ActualImages), \"images\")\n",
    "    \n",
    "    #saves the image\n",
    "    for i, (link, name) in enumerate(ActualImages):\n",
    "        try:\n",
    "            raw_img = urllib.request.urlopen(urllib.request.Request(link, headers=header)).read()\n",
    "            f = open(os.path.join(DIR, name + \".jpg\"), 'wb')\n",
    "            f.write(raw_img)\n",
    "            f.close()\n",
    "        except Exception as e:\n",
    "            print(\"could not load : \" + name)\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unquote' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-83173b540e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#added sleep to give a time delay of 10 seconds before query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mget_yandex_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'champagne flute'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_yandex_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'martini glass'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-8e855d7b1321>\u001b[0m in \u001b[0;36mget_yandex_soup\u001b[1;34m(searchterm)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'(?<=img_url\\=).*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munquote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'(\\&pos).*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unquote' is not defined"
     ]
    }
   ],
   "source": [
    "#added sleep to give a time delay of 10 seconds before query\n",
    "get_yandex_soup('champagne flute')\n",
    "sleep(10)\n",
    "get_yandex_soup('martini glass')\n",
    "sleep(10)\n",
    "get_yandex_soup('rummer glass')\n",
    "sleep(10)\n",
    "get_yandex_soup('snifters')\n",
    "sleep(10)\n",
    "get_yandex_soup('red wine glass')\n",
    "sleep(10)\n",
    "get_yandex_soup('white wine glass')\n",
    "sleep(10)\n",
    "get_yandex_soup('sherry glass')\n",
    "sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imgdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imgdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-f1e159e5b808>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-f1e159e5b808>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    google.py \"champagne flute\" -n 600 --interactive\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "google.py \"champagne flute\" -n 600 --interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "'chromedriver' executable not found. Download it from https://sites.google.com/a/chromium.org/chromedriver/downloads and place it next to this script file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m?\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mCHROME_DRIVER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     raise FileNotFoundError(f\"'chromedriver' executable not found. \"\n\u001b[0m\u001b[0;32m     24\u001b[0m                             \u001b[1;34mf\"Download it from {CHROME_DRIVER_DOWNLOAD_PAGE} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                             f\"and place it next to this script file\")\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: 'chromedriver' executable not found. Download it from https://sites.google.com/a/chromium.org/chromedriver/downloads and place it next to this script file"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "from imgdl import download\n",
    "from imgdl.settings import config\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "__file__ = ''\n",
    "\n",
    "IMAGE_STORE = Path(__file__).parent / 'images'\n",
    "\n",
    "CHROME_DRIVER = Path(__file__).parent / 'chromedriver'\n",
    "CHROME_DRIVER_DOWNLOAD_PAGE = \"https://sites.google.com/a/chromium.org/chromedriver/downloads\"\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "if not CHROME_DRIVER.exists():\n",
    "    raise FileNotFoundError(f\"'chromedriver' executable not found. \"\n",
    "                            f\"Download it from {CHROME_DRIVER_DOWNLOAD_PAGE} \"\n",
    "                            f\"and place it next to this script file\")\n",
    "\n",
    "\n",
    "def get_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "\n",
    "    if headless:\n",
    "        options.add_argument(\"headless\")\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        executable_path=str(CHROME_DRIVER),\n",
    "        options=options\n",
    "    )\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def scroll_down(driver, click_more_results=False):\n",
    "    if click_more_results:\n",
    "        smr = driver.find_element_by_id(\"smb\")\n",
    "        if smr.is_displayed():\n",
    "            smr.click()\n",
    "    else:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "\n",
    "def parse_urls_from_source(page_source):\n",
    "\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")\n",
    "    return [\n",
    "        json.loads(rg_di.find(\"div\", class_=\"rg_meta\").contents[0])[\"ou\"]\n",
    "        for rg_di in soup.find_all(\"div\", class_='rg_di')\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_urls(driver, n_images):\n",
    "    urls = parse_urls_from_source(driver.page_source)\n",
    "    previous_n = new_n = len(urls)\n",
    "\n",
    "    current_retries = 0\n",
    "    n_scrolls = 0\n",
    "    # Scroll down until there are enough images or unsuccessful retries exceeded maximum retries\n",
    "    while (new_n < n_images) and (current_retries < MAX_RETRIES):\n",
    "        scroll_down(\n",
    "            driver,\n",
    "            click_more_results=(new_n == previous_n) and (current_retries != 0)\n",
    "        )\n",
    "        n_scrolls += 1\n",
    "        print(f\"Scrolled {n_scrolls} times already\")\n",
    "        current_retries += 1\n",
    "        # Do incremental waits until more images appear\n",
    "        for i in range(4):\n",
    "            sleep(0.5 * i + 1)\n",
    "            urls = parse_urls_from_source(driver.page_source)\n",
    "            new_n = len(urls)\n",
    "            if new_n > previous_n:\n",
    "                current_retries = 0\n",
    "                print(f\"{new_n} images so far\")\n",
    "                break\n",
    "        previous_n = new_n\n",
    "\n",
    "    print(f\"{len(urls)} images found.\")\n",
    "    return urls\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    print(f\"Querying google images for '{args.query}'\")\n",
    "    driver = get_driver(headless=not args.interactive)\n",
    "    driver.get(\"https://images.google.com\")\n",
    "\n",
    "    elem = driver.find_element_by_name(\"q\")\n",
    "    elem.send_keys(args.query)\n",
    "    elem.send_keys(Keys.RETURN)\n",
    "\n",
    "    urls = get_urls(driver, args.n_images)\n",
    "\n",
    "    store_path = args.store_path / 'google' / args.query.replace(\" \", \"_\")\n",
    "    print(f\"Downloading to {store_path}\")\n",
    "    paths = download(\n",
    "        urls,\n",
    "        store_path=store_path,\n",
    "        n_workers=args.n_workers,\n",
    "        timeout=args.timeout,\n",
    "        min_wait=args.min_wait,\n",
    "        max_wait=args.max_wait,\n",
    "        proxies=args.proxy,\n",
    "        user_agent=args.user_agent,\n",
    "        notebook=args.notebook,\n",
    "        debug=args.debug,\n",
    "        force=args.force,\n",
    "    )\n",
    "\n",
    "    return dict(zip(urls, paths))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    "        description=\"Download images from a google images query\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument('query', type=str,\n",
    "                        help=\"Query string to be executed on google images\")\n",
    "\n",
    "    parser.add_argument('-n', '--n_images', type=int, default=100,\n",
    "                        help=\"Number of expected images to download\")\n",
    "\n",
    "    parser.add_argument('--interactive', action='store_true',\n",
    "                        help=\"Open up chrome interactively to see the search results and scrolling action.\")\n",
    "\n",
    "    parser.add_argument('-o', '--store_path', type=str, default=IMAGE_STORE,\n",
    "                        help=\"Root path where images should be stored\")\n",
    "\n",
    "    parser.add_argument('--n_workers', type=int, default=config['N_WORKERS'],\n",
    "                        help=\"Number of simultaneous threads to use\")\n",
    "\n",
    "    parser.add_argument('--timeout', type=float, default=config['TIMEOUT'],\n",
    "                        help=\"Timeout to be given to the url request\")\n",
    "\n",
    "    parser.add_argument('--min_wait', type=float, default=config['MIN_WAIT'],\n",
    "                        help=\"Minimum wait time between image downloads\")\n",
    "\n",
    "    parser.add_argument('--max_wait', type=float, default=config['MAX_WAIT'],\n",
    "                        help=\"Maximum wait time between image downloads\")\n",
    "\n",
    "    parser.add_argument('--proxy', type=str, action='append', default=config['PROXIES'],\n",
    "                        help=\"Proxy or list of proxies to use for the requests\")\n",
    "\n",
    "    parser.add_argument('-u', '--user_agent', type=str, default=config['USER_AGENT'],\n",
    "                        help=\"User agent to be used for the requests\")\n",
    "\n",
    "    parser.add_argument('-f', '--force', action='store_true',\n",
    "                        help=\"Force the download even if the files already exists\")\n",
    "\n",
    "    parser.add_argument('--notebook', action='store_true',\n",
    "                        help=\"Use the notebook version of tqdm\")\n",
    "\n",
    "    parser.add_argument('-d', '--debug', action='store_true',\n",
    "                        help=\"Activate debug mode\")\n",
    "\n",
    "    paths = main(parser.parse_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"paris by night\" -n 600 --interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Website Search (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
